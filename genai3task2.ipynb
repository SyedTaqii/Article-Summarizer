{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install datasets transformers accelerate evaluate rouge_score -q\n\nimport pandas as pd\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import T5Tokenizer\nimport warnings\n\n# Ignore simple warnings\n# warnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T17:47:27.713374Z","iopub.execute_input":"2025-11-01T17:47:27.713613Z","iopub.status.idle":"2025-11-01T17:47:33.618086Z","shell.execute_reply.started":"2025-11-01T17:47:27.713596Z","shell.execute_reply":"2025-11-01T17:47:33.617522Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# --- 2. Load and Sample the Dataset (Corrected) ---\n\n# Define the correct path to the folder containing the CSVs\nDATA_PATH = \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/\"\n\ndata_files = {\n    'train': DATA_PATH + 'train.csv',\n    'validation': DATA_PATH + 'validation.csv',\n    'test': DATA_PATH + 'test.csv'\n}\n\n# Load all three files into a DatasetDict\nprint(\"Loading all datasets...\")\nfull_dataset = load_dataset('csv', data_files=data_files)\nprint(\"Full dataset loaded:\")\nprint(full_dataset)\n\n# --- Create a Smaller Sample for Training ---\n# You can adjust this number. 50k is a good balance of speed and quality.\nTRAIN_SAMPLE_SIZE = 100000\n\nprint(f\"\\nCreating a training sample of {TRAIN_SAMPLE_SIZE} rows...\")\n\n# Create the final dataset we'll use for training\n# We need to clean up potential nulls from the CSV first\nclean_train = full_dataset['train'].filter(\n    lambda example: example['article'] is not None and example['highlights'] is not None\n)\n\ntokenized_dataset = DatasetDict({\n    'train': clean_train.shuffle(seed=42).select(range(TRAIN_SAMPLE_SIZE)),\n    'validation': full_dataset['validation'].filter(\n        lambda example: example['article'] is not None and example['highlights'] is not None\n    ),\n    'test': full_dataset['test'].filter(\n        lambda example: example['article'] is not None and example['highlights'] is not None\n    )\n})\n\nprint(\"Sampled and cleaned dataset created:\")\nprint(tokenized_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T17:48:15.330933Z","iopub.execute_input":"2025-11-01T17:48:15.331678Z","iopub.status.idle":"2025-11-01T17:48:55.458719Z","shell.execute_reply.started":"2025-11-01T17:48:15.331653Z","shell.execute_reply":"2025-11-01T17:48:55.456953Z"}},"outputs":[{"name":"stdout","text":"Loading all datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87f6b42ed1be415ba089d536a5633389"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d065b8a04b94aadb6859cea15b6bc58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"003cf28628ef4c329f5c0cd91248ebac"}},"metadata":{}},{"name":"stdout","text":"Full dataset loaded:\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'article', 'highlights'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['id', 'article', 'highlights'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['id', 'article', 'highlights'],\n        num_rows: 11490\n    })\n})\n\nCreating a training sample of 100000 rows...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3edf03aa86594d8495f2d1be2f503cef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80dc47b48d574bb89876e4aca48112cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31cc6a82d38d4abcac52a2509de28799"}},"metadata":{}},{"name":"stdout","text":"Sampled and cleaned dataset created:\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'article', 'highlights'],\n        num_rows: 100000\n    })\n    validation: Dataset({\n        features: ['id', 'article', 'highlights'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['id', 'article', 'highlights'],\n        num_rows: 11490\n    })\n})\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install --upgrade transformers huggingface_hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the T5 tokenizer\nMODEL_NAME = 't5-small'\nprint(f\"\\nLoading tokenizer for '{MODEL_NAME}'...\")\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n\n# T5 models require a task-specific prefix. For summarization, we use \"summarize: \"\nPREFIX = \"summarize: \"\n\n# We'll set max token lengths for the input and output\nMAX_INPUT_LENGTH = 512   # Max length for the article\nMAX_TARGET_LENGTH = 128  # Max length for the summary\n\ndef preprocess_function(examples):\n    \"\"\"\n    Prepares the data for T5.\n    1. Adds the \"summarize: \" prefix to the article.\n    2. Tokenizes the article (input).\n    3. Tokenizes the highlights (target/labels).\n    \"\"\"\n    \n    # Clean and prefix inputs (handle potential None/nan values)\n    inputs = [PREFIX + str(doc) for doc in examples['article']]\n    \n    # Tokenize the articles\n    model_inputs = tokenizer(\n        inputs, \n        max_length=MAX_INPUT_LENGTH, \n        truncation=True\n        # We don't pad here; the DataCollator will handle it (more efficient)\n    )\n\n    # Clean and tokenize the targets (summaries)\n    targets = [str(doc) for doc in examples['highlights']]\n    \n    # Tokenize labels using the 'as_target_tokenizer' context manager\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets, \n            max_length=MAX_TARGET_LENGTH, \n            truncation=True\n            # No padding here either\n        )\n\n    # Add the tokenized labels to our model inputs\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    return model_inputs\n\n# --- Apply the function to all splits ---\nprint(\"Tokenizing all datasets (this may take a few minutes)...\")\n\n# We use batched=True to process multiple examples at once (it's much faster)\n# We remove the old columns to save memory\ntokenized_dataset = tokenized_dataset.map(\n    preprocess_function, \n    batched=True, \n    remove_columns=['id', 'article', 'highlights']\n)\n\nprint(\"Tokenization complete.\")\nprint(tokenized_dataset)\n\n# Let's check a single processed example\nprint(\"\\n--- Example of one tokenized training item ---\")\nprint(tokenized_dataset['train'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T17:49:14.468158Z","iopub.execute_input":"2025-11-01T17:49:14.468917Z","iopub.status.idle":"2025-11-01T17:56:47.541340Z","shell.execute_reply.started":"2025-11-01T17:49:14.468890Z","shell.execute_reply":"2025-11-01T17:56:47.540710Z"}},"outputs":[{"name":"stdout","text":"\nLoading tokenizer for 't5-small'...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"786a45d94f9d4b44bff0e6f477755032"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ee488855c1e4f5b8d8a68e27b5ece06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3529cd08e6a742fd9b7187fc37373f0b"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"name":"stdout","text":"Tokenizing all datasets (this may take a few minutes)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"886d9bcf23c54864a687b8759d8784df"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db4ee78592743579b9ed1c2df897941"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64e1caa38d29408e958e24f7dcb18390"}},"metadata":{}},{"name":"stdout","text":"Tokenization complete.\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 100000\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 11490\n    })\n})\n\n--- Example of one tokenized training item ---\n{'input_ids': [21603, 10, 938, 3, 5, 11163, 63, 29, 2501, 15, 3, 5, 4946, 14925, 12177, 7, 33, 338, 3, 9, 11200, 53, 748, 24, 5689, 3, 9, 18936, 842, 5013, 53, 1067, 8, 643, 5, 309, 17344, 8, 458, 88, 1408, 16, 3, 9, 1367, 22, 6, 8, 1407, 5689, 8, 3640, 458, 9, 7591, 22, 45, 8, 798, 34, 19, 3641, 552, 34, 19, 2681, 16, 3, 9, 11095, 5, 3, 30705, 6, 66, 18936, 3640, 7, 33, 2681, 16, 3, 9, 1633, 18, 2689, 11, 3, 8623, 57, 3, 867, 12, 1709, 135, 3, 18687, 53, 30, 70, 2027, 344, 9612, 5, 933, 163, 405, 48, 4285, 149, 307, 3, 9, 18936, 842, 54, 36, 2697, 1067, 8, 643, 6, 68, 34, 1250, 8089, 12, 6570, 3, 99, 34, 19, 3255, 21, 15127, 5, 19063, 1342, 10, 37, 18190, 2686, 2149, 41, 5618, 134, 201, 1597, 57, 4946, 15789, 7, 6, 1250, 18936, 9812, 12, 916, 11850, 16, 3, 9, 1084, 18, 22588, 538, 1067, 8, 643, 383, 1855, 3, 5, 299, 147, 8, 657, 586, 767, 6, 12177, 7, 44, 3504, 15, 1846, 4457, 6, 4551, 7, 994, 6, 43, 4006, 91, 944, 842, 15127, 2673, 16, 84, 8, 1868, 1204, 46, 3640, 24, 141, 118, 20626, 11, 2657, 21, 15127, 257, 338, 8, 29009, 18190, 2686, 2149, 41, 5618, 134, 137, 94, 26032, 7, 8, 1124, 13, 8, 936, 643, 6, 5013, 53, 11035, 920, 1717, 1096, 8, 842, 78, 34, 54, 916, 12, 1681, 38, 34, 133, 1096, 3, 9, 840, 568, 5, 37, 748, 5386, 8, 97, 8, 3640, 54, 36, 6997, 1067, 8, 643, 12, 44, 709, 2641, 716, 6, 3, 2172, 28, 3, 9, 2411, 13, 386, 12, 662, 716, 30, 3, 867, 5, 100, 598, 9812, 54, 36, 3, 31340, 45, 856, 3, 9, 1846, 5, 3504, 15, 1846, 1553, 338, 8, 748, 16, 2083, 336, 215, 250, 13, 8, 381, 13, 3640, 7, 24, 12177, 7, 130, 5241, 12, 7198, 788, 12, 2357, 5, 37, 2833, 65, 437, 118, 3, 179, 12, 1845, 125, 33, 801, 38, 458, 1635, 19604, 40, 22, 3640, 7, 6, 84, 429, 1086, 43, 118, 3, 10863, 396, 1020, 63, 21, 8, 11095, 5, 6219, 16, 3, 9, 1367, 10, 37, 1407, 5689, 8, 3640, 3, 31, 9, 7591, 31, 45, 8, 798, 34, 19, 3641, 45, 3, 9, 18936, 31, 7, 643, 552, 34, 19, 2681, 16, 8, 643, 13, 3, 9, 11095, 3, 5, 37, 842, 16, 3, 9, 1367, 92, 1250, 12177, 7, 12, 6570, 823, 8, 18936, 842, 19, 3255, 21, 15127, 257, 5, 94, 19, 7501, 12, 6570, 3, 9, 529, 18, 346, 1014, 842, 11, 8176, 54, 36, 263, 81, 3237, 2020, 5, 37, 748, 65, 641, 19653, 1221, 45, 4281, 3, 9, 18936, 842, 24, 56, 59, 161, 11, 228, 474, 70, 280, 44, 1020, 5, 180, 19623, 7, 557, 456, 2101, 95, 3, 9, 1868, 12, 911, 3, 9, 18936, 842, 274, 8, 3640, 65, 4363, 250, 13, 8, 1643, 97, 79, 43, 12, 129, 34, 139, 8, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [18190, 2686, 2149, 5689, 842, 3, 31, 9, 7591, 31, 45, 18936, 12, 11095, 3, 5, 5879, 7, 143, 9812, 3853, 11, 3, 17454, 13418, 338, 18936, 1717, 3, 5, 18840, 7, 97, 3640, 54, 36, 1067, 3, 9, 643, 12, 44, 709, 2641, 716, 3, 5, 1]}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install evaluate\n!pip install rouge_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\nimport numpy as np\nimport torch\nfrom transformers import (\n    T5ForConditionalGeneration,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\n\n# --- 1. Load Model ---\n# We use T5ForConditionalGeneration, which includes the language modeling\n# head on top of the decoder, making it perfect for generation tasks.\nprint(f\"Loading model: '{MODEL_NAME}'...\")\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n\n# --- 2. Define Evaluation Metric (Deliverable 3) ---\n# We'll load the ROUGE metric from the 'evaluate' library\nprint(\"Loading ROUGE metric...\")\nrouge = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_pred):\n    \"\"\"\n    This function is called by the Trainer during evaluation.\n    It decodes the model's predictions and the true labels\n    and computes the ROUGE scores.\n    \"\"\"\n    predictions, labels = eval_pred\n    \n    # Decode predictions\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    \n    # In the labels, -100 is used for padding, so we must replace it\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    # Decode labels\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # ROUGE expects newline-separated sentences\n    decoded_preds = [\"\\n\".join(pred.strip()) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(label.strip()) for label in decoded_labels]\n\n    # Compute ROUGE\n    result = rouge.compute(\n        predictions=decoded_preds, \n        references=decoded_labels, \n        use_stemmer=True\n    )\n    \n    # Extract the main ROUGE scores\n    result = {key: value for key, value in result.items()}\n    \n    # Add mean generated length as a metric\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    \n    return {k: round(v, 4) for k, v in result.items()}\n\n# --- 3. Define Data Collator ---\n# This will dynamically pad all inputs and labels in a batch\n# to the same length. This is more efficient than padding everything\n# to 512 in the preprocessing step.\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model\n)\n\n# --- 4. Define Training Arguments ---\n# We use Seq2SeqTrainingArguments for encoder-decoder models\n# --- 4. Define Training Arguments ---\n# We use Seq2SeqTrainingArguments for encoder-decoder models\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./t5_summarization_results\", # Where to save checkpoints\n    \n    # --- Strategy ---\n    eval_strategy=\"epoch\",      # Run evaluation every epoch\n    save_strategy=\"epoch\",      # Save a checkpoint every epoch (matches eval)\n    \n    # --- Hyperparameters ---\n    learning_rate=1e-4,         # <-- INCREASED (T5 likes higher LR)\n    optim=\"adafactor\",          # <-- CHANGED (More memory efficient)\n    \n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    weight_decay=0.01,\n    num_train_epochs=3,\n    \n    # --- Checkpoint Management ---\n    save_total_limit=3,               # Only keep the 3 best checkpoints\n    load_best_model_at_end=True,    # <-- ADDED (Crucial for preventing overfitting)\n    metric_for_best_model=\"rouge1\", # <-- ADDED (Tells it to use ROUGE-1 as the decider)\n    \n    # --- Critical Flags ---\n    predict_with_generate=True,       \n    fp16=torch.cuda.is_available(),   \n    report_to=\"none\"                  \n)\n\n# --- 5. Initialize the Trainer ---\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics      # Pass our ROUGE function\n)\n\n# --- 6. Start Fine-Tuning ---\nprint(\"Starting fine-tuning...\")\ntrainer.train()\n\n# --- 7. Save the Final Model ---\nprint(\"Training complete. Saving final model...\")\nfinal_model_path = \"./t5_final_summarizer_model\"\ntrainer.save_model(final_model_path)\ntokenizer.save_pretrained(final_model_path)\n\nprint(f\"Model and tokenizer saved to {final_model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T17:57:04.827194Z","iopub.execute_input":"2025-11-01T17:57:04.827914Z","iopub.status.idle":"2025-11-01T21:23:04.515026Z","shell.execute_reply.started":"2025-11-01T17:57:04.827889Z","shell.execute_reply":"2025-11-01T21:23:04.514353Z"}},"outputs":[{"name":"stderr","text":"2025-11-01 17:57:09.022455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762019829.190434     105 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762019829.238371     105 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading model: 't5-small'...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67dc46aed07341229887ecc11620305e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be153e1ac5b74f88b10725509a57c6ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daf561201bb44a1b9b8efb21cca77e93"}},"metadata":{}},{"name":"stdout","text":"Loading ROUGE metric...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d0249d936b94e9a8139c812ce99fe94"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_105/3692663937.py:99: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting fine-tuning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='37500' max='37500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [37500/37500 3:25:35, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.008500</td>\n      <td>1.810303</td>\n      <td>0.427000</td>\n      <td>0.326300</td>\n      <td>0.332800</td>\n      <td>0.427000</td>\n      <td>19.998900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.980800</td>\n      <td>1.795307</td>\n      <td>0.426800</td>\n      <td>0.325800</td>\n      <td>0.332900</td>\n      <td>0.426800</td>\n      <td>19.999600</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.914900</td>\n      <td>1.789000</td>\n      <td>0.426300</td>\n      <td>0.325700</td>\n      <td>0.332800</td>\n      <td>0.426300</td>\n      <td>19.999400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Training complete. Saving final model...\nModel and tokenizer saved to ./t5_final_summarizer_model\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nfrom datasets import load_dataset\nimport textwrap\n\n# --- 1. Load the Fine-Tuned Model and Tokenizer ---\nprint(\"Loading final fine-tuned model and tokenizer...\")\nmodel_path = \"./t5_final_summarizer_model\"\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the model and tokenizer from the saved directory\nmodel = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\ntokenizer = T5Tokenizer.from_pretrained(model_path)\n\nprint(f\"Model loaded and on device: {device}\")\n\n# --- 2. Get a Few Samples from the Test Set ---\n# We'll use the 'full_dataset' variable that we loaded at the start\n# This way we can access the original, untokenized text.\ntest_samples = full_dataset['test'].shuffle(seed=42).select(range(5))\n\n# --- 3. Generate Summaries for Each Sample ---\nPREFIX = \"summarize: \"\n\nfor i, example in enumerate(test_samples):\n    original_article = example['article']\n    actual_summary = example['highlights']\n    \n    # Prepare the article for the model\n    input_text = PREFIX + original_article\n    \n    # Tokenize the article\n    inputs = tokenizer(\n        input_text, \n        max_length=512,  # Must match the training max_length\n        truncation=True, \n        return_tensors=\"pt\"\n    ).to(device)\n\n    # Generate the summary\n    with torch.no_grad():\n        output_ids = model.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_new_tokens=128,  # Max length for the generated summary\n            num_beams=4,          # Use beam search for higher quality\n            early_stopping=True\n        )\n    \n    # Decode the generated summary\n    generated_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    \n    # --- 4. Print the Comparison ---\n    print(\"=\" * 30)\n    print(f\"       EXAMPLE {i + 1}       \")\n    print(\"=\" * 30)\n    \n    # Use textwrap to make the long article more readable\n    print(\"\\n--- ðŸ“° ORIGINAL ARTICLE (truncated) ---\")\n    print(textwrap.fill(original_article, width=80))\n    \n    print(\"\\n--- ðŸŽ¯ ACTUAL SUMMARY ---\")\n    print(actual_summary)\n    \n    print(\"\\n--- ðŸ¤– MODEL'S GENERATED SUMMARY ---\")\n    print(generated_summary)\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:33:19.500834Z","iopub.execute_input":"2025-11-01T21:33:19.501542Z","iopub.status.idle":"2025-11-01T21:33:23.283301Z","shell.execute_reply.started":"2025-11-01T21:33:19.501511Z","shell.execute_reply":"2025-11-01T21:33:23.282378Z"}},"outputs":[{"name":"stdout","text":"Loading final fine-tuned model and tokenizer...\nModel loaded and on device: cuda\n==============================\n       EXAMPLE 1       \n==============================\n\n--- ðŸ“° ORIGINAL ARTICLE (truncated) ---\nKate Winslet was a vision in blue at a London film premiere this week. Her\nstunning body-con dress (top) had clearly been made to measure by Stella\nMcCartney. But my, what big feet â€” and big leopard-print stilettos â€” she has! At\n5 ft 7 in, the 39-year-old Oscar-winner is certainly no towering Amazon, but\nnonetheless she commands an out-of-the-ordinary UKÂ size-nine shoe. Kate is\nendearingly frank on the subject, telling interviewers that Titanic co-star\nLeonardo DiCaprio found the size of her feet hilarious: â€˜Iâ€™d put my foot up and\nheâ€™d fall about laughing because my feet are exactly the same size as his and\nheâ€™s 6 ft 1 in. Heâ€™d refer to them as my canoes!â€™ The average shoe size in the\nUK has risen from a dainty 4Â½ in 1900 to a roomy six today. But fascinatingly,\njust like Kate, lots of stars from Elle Macpherson to Gwyneth Paltrow all have\nsurprisingly huge feet, as we reveal here. Titanic feet: At 5ft 7in, the\n39-year-old Oscar-winning actress, who loves a strappy stiletto, wears a roomy\nsize-nine shoe . Bigfoot: Kate Winslet has revealed that her co-star Leonardo\nDiCaprio referred to her feet as 'my canoes' Jerry Hall Size: 8 (US 10) Elle\nMacpherson Size: 9.5 (US: 12) Katie Holmes Size: 8 (US: 11) The tall Texan, 58,\nis famous for her long legs and hanging onto Mick Jagger for 21 years. She said:\nâ€˜I have big feet, and they keep you firmly on the ground.â€™ The Aussie\nsupermodel, 51, is also a canny businesswoman. She markets her own brand of foot\nspa â€” perfect for feet of all sizes, not just Elleâ€™s sizeable twelves. Even at 5\nft 9in, Katie towered over her pint-sized former husband, Tom Cruise. The\n36-year-old actress does have large feet for her height - size 8. Scarlett\nJohansson Size: 7 (US: 9.5) Sandra Bullock Size: 7.5 (US: 10) Gwyneth Paltrow\nSize:Â 8 (US: 11) Sheâ€™s only 5 ft 3 in, but Scarlett, 30, has feet Â larger than\nthe average woman. The star admitted: â€˜I like my feet â€” I think theyâ€™re cute.â€™\nAmericaâ€™s sweetheart, 50, hasnâ€™t confirmed her shoe size, but is said to wear\n7.5 despite being only 5 ft 6 in. They certainly look big â€” and match her large\nhands. Gwynnie, 42, is known for her love of toweringly high heels â€” Michael\nKors, Brian Atwood and Giuseppe Zanotti. Her size eights Â seem large on her 5 ft\n7 inframe. Angelina JolieÂ Size:Â 6.5 (US: 9) Uma Thurman Size: 8 (US: 11) Cate\nBlanchett Size: 6 (US: 9) Her size nines were derided as â€˜weirdâ€™ and â€˜giganticâ€™\nafter Jolie, 39, appeared barefoot in a Louis Vuitton campaign. At 5 ft 7in, her\ntiny figure emphasises her shoe size. Uma, 44, is a modest 5 ft 11in. She says:\nâ€˜Iâ€™ve always felt too tall, with big feet.â€™ Quentin Tarantino disagreed, and\ninsisted on close-ups of her size eight feet in Pulp Fiction and Kill Bill. At 5\nft 9 in, Cate, 45, struggles to walk in her shoes at times â€” shedding\nherÂ Givenchys at a press conference, revealing bunions and red, pinched toes.\n\n--- ðŸŽ¯ ACTUAL SUMMARY ---\nKate Winslet wears size nine shoes and her Titanic co-star Leonardo DiCaprio found the size of her shoes hilarious .\nScarlett Johansson, Gwyneth Paltrow, Sandra Bullock, Angelina Jolie, and Cate Blanchett all have huge feet .\nJerry Hall, Elle Macpherson, Katie Holmes, and Uma Thurman also have feet bigger the average UK shoe size .\n\n--- ðŸ¤– MODEL'S GENERATED SUMMARY ---\nThe 39-year-old actress wears a size-nine shoe at 5ft 7in. The average shoe size in the UK has risen from a dainty 412 in 1900 to a roomy six today. The average shoe size in the UK has risen from a dainty 412 in 1900 to a roomy six today.\n\n\n==============================\n       EXAMPLE 2       \n==============================\n\n--- ðŸ“° ORIGINAL ARTICLE (truncated) ---\n(CNN)For 12 years Adelma Cifuentes felt worthless, frightened and alone, never\nknowing when her abusive husband would strike. But as a young mother in rural\nGuatemala with three children and barely a third grade education, she thought\nthere was no way out. What began as psychological torment, name-calling and\nhumiliation turned into beatings so severe Cifuentes feared for her life. One\nday, two men sent by her husband showed up at her house armed with a shotgun and\norders to kill her. They probably would have succeeded, but after the first\nbullet was fired, Cifuentes' two sons dragged her inside. Still, in her deeply\nconservative community, it took neighbors two hours to call for help and\nCifuentes lost her arm. But the abuse didn't stop there. When she returned home,\nCifuentes' husband continued his attacks and threatened to rape their little\ngirl unless she left. That's when the nightmare finally ended and her search for\njustice began. Cifuentes' case is dramatic, but in Guatemala, where nearly 10\nout of every 100,000 women are killed, it's hardly unusual. A 2012 Small Arms\nSurvey says gender-based violence is at epidemic levels in Guatemala and the\ncountry ranks third in the killings of women worldwide. According to the United\nNations, two women are killed there every day. There are many reasons why,\nbeginning with the legacy of violence left in place after the country's 36-year-\nold civil war. During the conflict, atrocities were committed against women, who\nwere used as a weapon of war. In 1996, a ceasefire agreement was reached between\ninsurgents and the government. But what followed and what remains is a climate\nof terror, due to a deeply entrenched culture of impunity and discrimination.\nMilitary and paramilitary groups that committed barbaric acts during the war\nwere integrated back into society without any repercussions. Many remain in\npower, and they have not changed the way they view women. Some 200,000 people\nwere either killed or disappeared during the decades-long conflict, most of them\nfrom indigenous Mayan populations. Nearly 20 years later, according to the\nSecurity Sector Reform Resource Centre, levels of violent crime are  higher in\nGuatemala than they were during the war. But despite the high homicide rate, the\nUnited Nations estimates 98% of cases never make it to court. Women are\nparticularly vulnerable because of a deep-rooted gender bias and culture of\nmisogyny. In many cases, femicide -- the killing of a woman simply because of\nher gender -- is carried out with shocking brutality with some of the same\nstrategies used during the war, including rape, torture and mutilation. Mexican\ndrug cartels, organized criminal groups and local gangs are contributing to the\nvicious cycle of violence and lawlessness.  Authorities investigating drug-\nrelated killings are stretched thin, leaving fewer resources to investigate\nfemicides. In many cases, crime is not reported because of fear of retaliation.\nMany consider the Guatamalen National Civil Police, or PNC, corrupt, under-\nresourced and ineffective. Even if a case does get prosecuted, according to\nHuman Rights Watch, the country's weak judicial system has proved incapable of\nhandling the explosion in violence. Perhaps one of the biggest challenges facing\nwomen in Guatemala is the country's deeply rooted patriarchal society. According\nto MarÃ­a Machicado TerÃ¡n, the representative of U.N. women in Guatemala, \"80% of\nmen believe that women need permission to leave the house, and 70% of women\nsurveyed agreed.\" This prevailing culture of machismo and an institutionalized\nacceptance of brutality against women leads to high rates of violence. Rights\ngroups say machismo not only condones violence, it places the blame on the\nvictim. The political will to address violence against women is slow to\nmaterialize. \"Politicians don't think women are important,\" says former\nSecretary General of the Presidential Secretariat for Women Elizabeth Quiroa.\n\"Political parties use women for elections. They give them a bag of food and\npeople sell their dignity for this because they are poor.\" Lack of education is\na major contributor to this poverty. Many girls, especially in indigenous\ncommunities don't go to school because the distance from their house to the\nclassroom is too far. Quiroa says \"They are subject to rape, violence and forced\nparticipation in the drug trade.\" Although the situation for girls and women in\nGuatemala is alarming, there are signs the culture of discrimination may be\nslowly changing.  With the help of an organization known as CICAM, or Centro de\nInvestigaciÃ³n, Cifuentes was finally able to escape her  husband and get the\njustice she deserved. He is now spending 27 years behind bars. Cifuentes is\nusing her painful past to provide hope and healing to others through art. Since\n2008, she and four other abuse survivors known as La Poderosas, or \"The\nPowerful,\" have been appearing in a play based on their real life stories. The\nshow not only empowers other women and discusses the problem of violence openly,\nbut it also offers suggestions for change. And it's having an impact. Women have\nstarted breaking their silence and asking where they can get support. Men are\nreacting, too. One of the main characters, Lesbia TÃ©llez, says  during one\npresentation, a man stood up and started crying when he realized how he had\ntreated his wife and how his mother had been treated. He said he wanted to be\ndifferent. The taboo topic of gender-based violence is also being acknowledged\nand recognized in a popular program targeting one of Guatemala's most vulnerable\ngroups, indigenous Mayan girls.  In 2004, with help from the United Nations and\nother organizations, the Population Council launched a community-based club\nknown as Abriendo Oportunidades, or \"Opening Opportunities\". The goal is to\nprovide girls with a safe place to learn about their rights and reach their full\npotential. Senior Program Coordinator Alejandra Colom says the issue of violence\nis discussed and girls are taught how to protect themselves. \"They then share\nthis information with their mothers and for the first time, they realize they\nare entitled to certain rights.\" Colom adds that mothers then become invested in\nsending their daughters to the clubs and this keeps them more visible and less\nprone to violence. The Guatemalan government is also moving in the right\ndirection to address the problem of violence against women. In 2008, the\nCongress passed a law against femicide. Two years later the attorney general's\noffice created a specialized court to try femicides and other violent crimes\nagainst women. In 2012, the government established a joint task force for crimes\nagainst women, making it easier for women to access justice by making sure\nvictims receive the assistance they need. The government has also established a\nspecial 24-hour court to attend to femicide cases. On the global front, the\nInternational Violence Against Women Act was introduced in the U.S. Congress in\n2007; it has been pending ever since. But last week the act was reintroduced in\nboth the House and Senate. If approved, it would make reducing levels of gender-\nbased violence a U.S. foreign policy priority. Pehaps the most immediate and\neffective help is coming from International nongovernmental organizations, which\nare on the front lines of the fight against gender-based discrimination in\nGuatemala. Ben Weingrod, a senior policy advocate at the global poverty fighting\ngroup CARE, says, \"We work to identify and challenge harmful social norms that\nperpetuate violence.  Our work includes engaging men and boys as champions of\nchange and role models, and facilitating debates to change harmful norms and\ncreate space for more equitable relationships between men and women.\" But the\njob is far from over. While there is tempered optimism and hope for change, the\nproblem of gender-based violence in Guatemala is one that needs international\nattention and immediate action. Cifuentes is finding strength through the\ntheater and the support of other abuse survivors, which has allowed her to move\nforward. But millions of other women trapped in a cycle of violence are facing\ndangerous and frightening futures. For them, it's a race against time and help\ncannot come soon enough.\n\n--- ðŸŽ¯ ACTUAL SUMMARY ---\nGender-based  violence is at epidemic levels in Guatemala .\nAccording to the United Nations, two women are killed in Guatemala every day .\nFive abuse survivors known as La Poderosas have been appearing in a play based on their real life stories .\n\n--- ðŸ¤– MODEL'S GENERATED SUMMARY ---\nA 2012 Small Arms Survey says gender-based violence is at epidemic levels in Guatemala. The country ranks third in the killings of women worldwide.\n\n\n==============================\n       EXAMPLE 3       \n==============================\n\n--- ðŸ“° ORIGINAL ARTICLE (truncated) ---\nKabul, Afghanistan (CNN)The flag is crude, handmade, but the message is clear --\nallegiance to ISIS in Afghanistan. And the timing -- with America withdrawing,\nthe Taliban fractured, young men disillusioned and angry -- could not be worse.\nA group of fighters in Afghanistan agreed to be filmed by a CNN cameraman\nparading their ISIS flags in a valley not far to the south of Kabul, the Afghan\ncapital. They are the first images of their kind shot by western media inside\nAfghanistan. The rise of ISIS is an issue that the Afghan President, Ashraf\nGhani, has termed a \"terrible threat.\" U.S. officials CNN has spoken to have\nvoiced their concern about the potential for an ISIS presence. One U.S. military\nofficer said the militants currently have limited capability but are trying to\nrecruit disillusioned Taliban in several areas around the country's east and\nsouth. \"There has been some very small numbers of recruitment that has\nhappened,\" Colonel J B Vowell, told CNN. \"You have disaffected Taliban who are\nlosing politically and some of the younger, newer fighters are moving to that\ncamp. It doesn't mean it's operationally better. We are concerned about it --\nresources, weapons, capabilities. (But) I don't see an operational effect.\" In\nthe valley, the men display their weapons, and practice high kicks. They are a\nlittle breathless at altitude, a little clumsy. They are all masked, all in\nmilitary-style uniforms. Our cameraman described how locals seemed to keep their\ndistance from them. It is often said that rivalry between the nascent ISIS\npresence and the Taliban, who remain the big guns in Afghanistan, is fierce\nenough to mean the ISIS fighters could be killed for brandishing the flag. But\nit is fatigue with the Taliban that appears to have provided fertile ground for\ntheir rise. One of them told CNN: \"We established contacts with IS (another\nacronym for the group) through a friend who is in Helmand (in southern\nAfghanistan). \"He called us, saying: 'the IS people have come to Afghanistan --\nlet's join them.' Then we joined them and pledged allegiance to them.\" Our\ncameraman wasn't allowed to film the satellite phones they say they use to talk\nto Iraq and Syria. They said they were religious students and deny any former\nassociation with the Taliban. They said that at night they go into nearby\nvillages to try and find yet more recruits. They watch a mixture of online\npropaganda, old and new, on their smartphones. The fighter went on to explain\nthat they were currently talking to the Taliban to determine whether they would\nwork with or rival them.  He added they are currently designating a new leader,\nafter the supposed head of ISIS in Afghanistan, Abdul Rauf Khadim, was\nreportedly killed in a drone strike earlier in the year. For months, the Afghan\ngovernment played down the threat of a looming ISIS presence in Afghanistan, yet\nduring his recent trip to Washington, Ghani struck a different tone. \"We are the\nfront line. The terrorists neither recognize boundaries nor require passports to\nspread their message of hate and discord. From the west, Daesh is already\nsending advance guards to southern and western Afghanistan to push our\nvulnerabilities,\" he told U.S. Congress in late March, using the pejorative name\nused to describe the militants by many ISIS opponents in the region. There is\nsome evidence to suggest that ISIS may already be operating in the country. A\nseries of brutal attacks on civilian buses have baffled investigators in the\npast month. The first was in February, when 30 people from the Hazara ethnic\ngroup -- Shia Muslims -- were abducted from a bus near Zabul province in the\nsouth of the country. They have yet to return. Another hit three buses traveling\nin Wardak, central Afghanistan, killing 13 civilians including women and\nchildren. Suspicions have fallen on possible nascent ISIS cells as the Taliban\nhave vehemently denied responsibility for the attacks. Khalil Andrabi, the\npolice chief of Wardak told CNN of the bus attack: \"I can't hundred percent say\nthat they were IS, but their act was completely similar to what IS is doing in\nSyria (and) Iraq.\" Solid info on ISIS' whereabouts in the country is hard to\ncome by. CNN spoke to local officials from five regions -- some emphasized the\ngrowing threat of the terror group, while others played it down. Zabul: MP Abdul\nQader Qalatwal says: \"People have seen foreigners from central Asian countries\nand Arab countries wearing black clothes and masks and having black flags in the\ndistricts of Khak Afghan and in parts of Arghandab.\" \"Those foreigners are rich,\neven carrying U.S. dollars. They have weapons and vehicles. Some of them have\neven brought their families.\" Nangarhar: MP Esmatullah Shinwari says: \"According\nto some reports, black flags have been seen in Nangarhar's Haska Mina district\n-- and a former Taliban local commander, Abdul Khaleq, is now claiming to be\nISIS' representative in that district.\" Farah: Senator Haji Gul Ahmad Azimi\nsays: \"According to the reports I have received from local officials in Farah, a\nnumber of foreign fighters -- including women -- have been seen in the district\nof Khak Safid, wearing mostly black clothes, and some [with] the Arabic\nheadscarf. They have good vehicles and they are rich [enough] to buy food or\ngoods at local shops for twice the normal value.\" \"They are said to live in the\nmountainous areas of the Khak Safid district in abandoned mud houses, and a\nmonth ago were rumored to be training in the area. I cannot 100% confirm they\nare ISIS, however.\" Wardak: MP Shir Wali Wardak says: \"I don't think ISIS\nfighters from Syria and Iraq have come here to Afghanistan -- but hardcore\nTaliban members who have understood that the Taliban name is dying have changed\nthe color of their flags from white to black in order to stay alive. I know that\nsome black flags have been seen in Wardak province, raised by ex-Taliban\nfighters.\" Ghazni: Deputy Governor Mohammad Ali Ahmadi says: \"There are ex-\nTaliban fighters operating under the name of ISIS in Ghazni province at the\nmoment who have changed their flag from white to black. There have been armed\nclashes between newly-converted ISIS (members) and Taliban fighters ... who\nshould be in control of certain places.\" CNN's Masoud Popalzai contributed to\nthis report.\n\n--- ðŸŽ¯ ACTUAL SUMMARY ---\nA group of fighters in Afghanistan is filmed by a CNN cameraman parading ISIS flags .\nU.S. official: ISIS militants have \"no military capability\" at present, but are trying to recruit disillusioned Taliban in several areas .\nRivalry between ISIS and the Taliban in Afghanistan is fierce enough to mean the ISIS fighters could be killed for brandishing the flag .\n\n--- ðŸ¤– MODEL'S GENERATED SUMMARY ---\nA group of fighters in Afghanistan agreed to be filmed by a CNN cameraman parading their ISIS flags in a valley not far to the south of Kabul. U.S. officials CNN has spoken to have voiced their concern about the potential for an ISIS presence.\n\n\n==============================\n       EXAMPLE 4       \n==============================\n\n--- ðŸ“° ORIGINAL ARTICLE (truncated) ---\n(CNN)Suzanne Crough, the child actress who portrayed youngest daughter Tracy on\nthe '70s musical sitcom \"The Partridge Family,\" has died. She was 52. Crough\npassed away Monday at home in Laughlin, Nevada, the Clark County Coroner's\nOffice said. Tracy played tambourine and percussion in the traveling \"Partridge\nFamily\" band. The group consisted of a widowed mom, played by Shirley Jones, and\nher five children, played by David Cassidy, Susan Dey, Danny Bonaduce, Brian\nForster and Crough. Band manager Reuben Kincaid, played by Dave Madden, rounded\nout the cast. The band had real hit songs with \"Come On Get Happy\" and \"I Think\nI Love You,\" though not all the members really sang or played instruments. The\nshow aired from 1970-74. People we've lost in 2015 . Redheaded Crough was raised\nin Los Angeles, the youngest of eight children, according to The Hollywood\nReporter. Crough also starred in the TV series \"Mulligan's Stew\" and had spots\non other series in the '70s. She appeared in a \"Partridge Family\" reunion on the\n\"Today\" show in 2010. \"I'm an office manager for Office Max,\" she told host Matt\nLauer. \"I have two daughters, I'm married, I have a normal job.\" CNN's Henry\nHanks contributed to this report.\n\n--- ðŸŽ¯ ACTUAL SUMMARY ---\nSuzanne Crough was the youngest member of TV's \"Partridge Family\"\nCrough died Monday at 52 in Nevada .\n\n--- ðŸ¤– MODEL'S GENERATED SUMMARY ---\nSuzanne Crough was raised in Los Angeles, the youngest of eight children. She appeared in a \"Partridge Family\" reunion on the \"Today\" show in 2010.\n\n\n==============================\n       EXAMPLE 5       \n==============================\n\n--- ðŸ“° ORIGINAL ARTICLE (truncated) ---\nNewcastle will be keen to avoid a third defeat on the spin when they take on\nrivals Sunderland in Sundayâ€™s crunch derby after floundering in recent weeks\nwithout banned striker Papiss Cisse. It is Dick Advocaatâ€™s side who are\nstruggling at the foot of the Barclays Premier League table â€“ sitting perilously\none point above the bottom three - but Sportsmail can reveal that John Carverâ€™s\nteam would be two points ADRIFT of their foes in the relegation zone if it\nwerenâ€™t for Cisseâ€™s goals. The Senegal forward is Newcastleâ€™s top scorer this\nseason with 11 goals, with 10 of those decisive in changing games in the\nMagpiesâ€™ favour. Papiss Cisse is Newcastle's top scorer this season with 11\ngoals - 10 of those were decisive . 20 September Hull (h) 2-2 (2 goals) 4\nOctober Swansea (a) 2-2 (2 goals) 2 December Burnley (a) 1-1 (1 goal) 6 December\nChelsea (h) 2-1 (2 goals) 26 December Man Utd (a) 1-3 (1 goal) 28 December\nEverton (h) 3-2 (1 goal) 11 February Crystal Palace (a) 1-1 (1 goal) 28 February\nAston Villa (h) (1 goal) Since his double against Hull rescued a 2-2 draw in\nSeptember, Cisse has earned Newcastle 11 points with his goalscoring heroics â€”\nalmost single-handedly lifting them to mid-table. Take those goals away and the\nPremier League reads somewhat differently. Newcastle languish in 18th place on\n24 points in the fantasy table behind Sunderland on 26. They would have scored\njust 22 times â€” the second fewest in the division behind Aston Villa â€” and won\nonly six games. Six of Cisseâ€™s strikes have been against sides in the bottom\nhalf and Burnley, Hull, Everton, and Villa would have leapfrogged a Newcastle\nside shorn of the striker. After his brace against Steve Bruceâ€™s side, Cisse\nscored another double to clinch a 2-2 draw against Swansea two weeks later. His\nnext goal was a second-half equaliser in a 1-1 with Burnley in December and,\nfour days later, hit another two to seal a 2-1 victory over Chelsea. Later that\nmonth he netted in a 3-2 triumph over Everton while in February he scored\nagainst his former manager Alan Pardew at Crystal Palace in a 1-1 draw. Cisse\nalso supplied the cutting edge to snatch a 1-0 victory against Villa. The\nSenegalese striker is currently serving a seven-game ban after clashing with\nJonny Evans . 5 April Sunderland (a) 13 April Liverpool (a) 19 April Tottenham\n(h) 25 April Swansea (h) 2 May Leicester (a) His only goal which hasnâ€™t had\naffected the outcome of a match was a late consolation penalty in a 3-1 defeat\nat Old Trafford on Boxing Day. Even when Cisse hasnâ€™t found the net, however, he\nstill has a beneficial impact. Newcastle have a points-to-games ratio of 1.37\nwhen the striker plays but just 0.9 when he doesnâ€™t. In reality, there is little\ncause for concern as Carverâ€™s side currently sit 10 points clear of the drop\nzone. But after their trip to the Stadium of Light they will face top-four\nchasing Liverpool and Tottenham in the coming weeks. Newcastle will be hoping\nthey arenâ€™t looking over their shoulders when Cisse returns for their final\nthree games of the campaign. Cisse nets against Burnley at Turf Moor, a goal\nthat saw Newcastle earn a point in December .\n\n--- ðŸŽ¯ ACTUAL SUMMARY ---\nNewcastle hope to avoid third defeat in a row when they face Sunderland .\nPapiss Cisse has scored 11 Premier League goals this season .\n10 of those have been decisive, and the Magpies would be in the relegation zone behind Sunderland without his goals .\n\n--- ðŸ¤– MODEL'S GENERATED SUMMARY ---\nPapiss Cisse is Newcastle's top scorer this season with 11 goals. 10 of those decisive in changing games in the Magpies' favour. Newcastle languish in 18th place on 24 points in the fantasy table behind Sunderland on 26.\n\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nimport textwrap\n\n# --- 1. Load the Fine-Tuned Model and Tokenizer ---\nprint(\"Loading final fine-tuned model and tokenizer...\")\nMODEL_PATH = \"./t5_final_summarizer_model\"\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the model and tokenizer from the saved directory\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(DEVICE)\ntokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)\n\nprint(f\"Model loaded and on device: {DEVICE}\")\n\ndef summarize_article(article_text, max_summary_length=150):\n    \"\"\"\n    Generates a summary for a given article text.\n    \"\"\"\n    \n    # 1. Prepare the article for the model\n    # We must use the same prefix T5 was trained on\n    PREFIX = \"summarize: \"\n    input_text = PREFIX + article_text\n    \n    # 2. Tokenize the article\n    print(\"Tokenizing input text...\")\n    inputs = tokenizer(\n        input_text, \n        max_length=512,  # Max input length (from training)\n        truncation=True, \n        return_tensors=\"pt\"\n    ).to(DEVICE)\n\n    # 3. Generate the summary\n    print(\"Generating summary...\")\n    with torch.no_grad():\n        output_ids = model.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],\n            max_new_tokens=max_summary_length,\n            num_beams=4,          # Use beam search\n            early_stopping=True\n        )\n    \n    # 4. Decode the generated summary\n    generated_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    \n    return generated_summary\n\n# --- Example Test ---\n\n# Paste any long article text here\nmy_article = \"\"\"\nLee hosted Xi at a state summit and dinner after an annual summit of the Asia-Pacific Economic Cooperation (Apec) in the South Korean city of Gyeongju, marking Xiâ€™s first visit to the United Statesâ€™ ally in 11 years.\nBeijing attaches great importance to relations with Seoul and sees South Korea as an inseparable cooperative partner, Xi said ahead of the summit, according to Leeâ€™s office.\nLee, who was elected president in a snap election in June, has promised to strengthen ties with the US while not antagonising China and seeking to reduce tensions with the North.\nâ€œI am very positive about the situation in which conditions for engagement with North Korea are being formed,â€ Lee said, referring to recent high-level exchanges between China and North Korea.\nâ€œI also hope that South Korea and China will take advantage of these favourable conditions to strengthen strategic communication to resume dialogue with North Korea.â€\nLee has called for a phased approach to denuclearising North Korea, starting with engagement and a freeze on further development of nuclear weapons.\nIn a statement on Saturday, Pyongyang, a military and economic ally of China, dismissed the denuclearisation agenda as an unrealisable â€œpipe dreamâ€.\nNorth Korea has repeatedly and explicitly rejected Leeâ€™s overtures, saying it will never talk to the South. In recent years Pyongyang abandoned its longstanding policy of unification with the South and called Seoul a main enemy.\nLeader Kim Jong Un said he would be willing to talk to the US if Washington drops demands for denuclearisation, but he did not publicly respond when US President Donald Trump offered talks during his visit to South Korea earlier this week.\nTrump and Lee announced a surprise breakthrough in talks to lower US tariffs in return for billions of dollars in investment from South Korea. The US president then departed before the main Apec leadersâ€™ summit.\nSouth Korean national security adviser Wi Sunglac told a briefing that China expressed its willingness to cooperate for peace and stability on the Korean peninsula, but the leaders did not specifically discuss what kind of role China would play.\nBoth sides also agreed that the US-North Korea dialogue was most important, the adviser said.\nChinese state media reports on the meeting with Lee made no mention of the North Korea discussions.\nAccording to Xinhua, Xi proposed ways to open a new chapter in relations, including having each country â€œrespect each others social systems and development paths, accommodate core interests and major concerns, and properly handle differences through friendly consultationâ€œ.\nXi also called for upholding multilateralism and increasing cooperation in areas such as artificial intelligence, biopharmaceuticals, green industries and aging populations, Xinhua reported.\nDuring Xiâ€™s visit, China and South Korea signed seven agreements including a won-yuan currency swap and memorandums of understanding on online crime, businesses that cater to aging populations, and innovation, among other issues.\n\"\"\"\n\n# Generate the summary\nmy_summary = summarize_article(my_article)\n\n# Print the results\nprint(\"\\n\" + \"=\"*30)\nprint(\"       YOUR TEST SUMMARY       \")\nprint(\"=\"*30)\nprint(\"\\n--- ORIGINAL ARTICLE ---\")\nprint(textwrap.fill(my_article, width=80))\nprint(\"\\n--- ðŸ¤– GENERATED SUMMARY ---\")\nprint(my_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:36:58.331048Z","iopub.execute_input":"2025-11-01T21:36:58.331773Z","iopub.status.idle":"2025-11-01T21:36:59.354245Z","shell.execute_reply.started":"2025-11-01T21:36:58.331748Z","shell.execute_reply":"2025-11-01T21:36:59.353395Z"}},"outputs":[{"name":"stdout","text":"Loading final fine-tuned model and tokenizer...\nModel loaded and on device: cuda\nTokenizing input text...\nGenerating summary...\n\n==============================\n       YOUR TEST SUMMARY       \n==============================\n\n--- ORIGINAL ARTICLE ---\n Lee hosted Xi at a state summit and dinner after an annual summit of the Asia-\nPacific Economic Cooperation (Apec) in the South Korean city of Gyeongju,\nmarking Xiâ€™s first visit to the United Statesâ€™ ally in 11 years. Beijing\nattaches great importance to relations with Seoul and sees South Korea as an\ninseparable cooperative partner, Xi said ahead of the summit, according to Leeâ€™s\noffice. Lee, who was elected president in a snap election in June, has promised\nto strengthen ties with the US while not antagonising China and seeking to\nreduce tensions with the North. â€œI am very positive about the situation in which\nconditions for engagement with North Korea are being formed,â€ Lee said,\nreferring to recent high-level exchanges between China and North Korea. â€œI also\nhope that South Korea and China will take advantage of these favourable\nconditions to strengthen strategic communication to resume dialogue with North\nKorea.â€ Lee has called for a phased approach to denuclearising North Korea,\nstarting with engagement and a freeze on further development of nuclear weapons.\nIn a statement on Saturday, Pyongyang, a military and economic ally of China,\ndismissed the denuclearisation agenda as an unrealisable â€œpipe dreamâ€. North\nKorea has repeatedly and explicitly rejected Leeâ€™s overtures, saying it will\nnever talk to the South. In recent years Pyongyang abandoned its longstanding\npolicy of unification with the South and called Seoul a main enemy. Leader Kim\nJong Un said he would be willing to talk to the US if Washington drops demands\nfor denuclearisation, but he did not publicly respond when US President Donald\nTrump offered talks during his visit to South Korea earlier this week. Trump and\nLee announced a surprise breakthrough in talks to lower US tariffs in return for\nbillions of dollars in investment from South Korea. The US president then\ndeparted before the main Apec leadersâ€™ summit. South Korean national security\nadviser Wi Sunglac told a briefing that China expressed its willingness to\ncooperate for peace and stability on the Korean peninsula, but the leaders did\nnot specifically discuss what kind of role China would play. Both sides also\nagreed that the US-North Korea dialogue was most important, the adviser said.\nChinese state media reports on the meeting with Lee made no mention of the North\nKorea discussions. According to Xinhua, Xi proposed ways to open a new chapter\nin relations, including having each country â€œrespect each others social systems\nand development paths, accommodate core interests and major concerns, and\nproperly handle differences through friendly consultationâ€œ. Xi also called for\nupholding multilateralism and increasing cooperation in areas such as artificial\nintelligence, biopharmaceuticals, green industries and aging populations, Xinhua\nreported. During Xiâ€™s visit, China and South Korea signed seven agreements\nincluding a won-yuan currency swap and memorandums of understanding on online\ncrime, businesses that cater to aging populations, and innovation, among other\nissues.\n\n--- ðŸ¤– GENERATED SUMMARY ---\nBeijing attaches great importance to relations with Seoul and sees South Korea as an inseparable cooperative partner. Lee has promised to strengthen ties with the US while not antagonising China. Pyongyang dismissed the denuclearisation agenda as an unrealisable â€œpipe dreamâ€\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!zip -r my_t5_model.zip /kaggle/working/t5_final_summarizer_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:31:41.803869Z","iopub.execute_input":"2025-11-01T21:31:41.804502Z","iopub.status.idle":"2025-11-01T21:31:54.236546Z","shell.execute_reply.started":"2025-11-01T21:31:41.804477Z","shell.execute_reply":"2025-11-01T21:31:54.235588Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/t5_final_summarizer_model/ (stored 0%)\n  adding: kaggle/working/t5_final_summarizer_model/model.safetensors (deflated 7%)\n  adding: kaggle/working/t5_final_summarizer_model/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/t5_final_summarizer_model/special_tokens_map.json (deflated 85%)\n  adding: kaggle/working/t5_final_summarizer_model/training_args.bin (deflated 52%)\n  adding: kaggle/working/t5_final_summarizer_model/spiece.model (deflated 48%)\n  adding: kaggle/working/t5_final_summarizer_model/added_tokens.json (deflated 83%)\n  adding: kaggle/working/t5_final_summarizer_model/generation_config.json (deflated 28%)\n  adding: kaggle/working/t5_final_summarizer_model/config.json (deflated 63%)\n","output_type":"stream"}],"execution_count":5}]}